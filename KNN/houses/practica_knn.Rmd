---
title: "PracticaKNN"
author: "Hernán Darío Vanegas Madrigal"
date: "9/23/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Práctica KNN

Lectura de la base de datos

```{r cars}
# Libreria que permite cargar documentos de R
library(tidyverse)

# Metodo para quedarse con las columnas que yo quiero
price <- read_csv2("real_estate_valuation_data_set.csv")

# Reducción
price_reduction <- price[, c("X2", "X3", "X4", "Y")]
```

Limpieza de datos y corrección nombre de columnas

```{r}
# Analisis descriptivo
summary(price_reduction)

# Calculo de la desviacion estándar, cuando pongo 2 hago referencia a las filas
apply(price_reduction, 2, sd)

# Graficas
pairs(price_reduction)
```

Calculo de la correlación de las variables:

```{r}
cor(price_reduction)
```

```{r}
plot(
  price_reduction$X3,
  price_reduction$Y,
  las = 1,
  xlab = "Distancia al MRT [mts]",
  ylab = "Precio de la casa [$]",
)
```
```{r}
boxplot(
  Y ~ X4,
  data = price_reduction,
  xlab = "Number of convenience stores",
  ylab = "House price [$]"
)
```

## Implementación del método de los K vecinos mas cercanos

### Partición de los datos en entrenamiento y validación:

Se cuenta con `r dim(price_reduction)[1]` registros. Se tomará el 25% de estos datos para validación.

```{r}
# Se fija la semilla para obtener los mismos aleatorios
set.seed(2019092)

number_records <- dim(price_reduction)[1]
number_records_validation <- round(number_records * 0.25)

indexes_validation <- sample(1:number_records, number_records_validation, replace = FALSE)

# Variables independientes
X_train <- price_reduction[-indexes_validation, c("X2", "X3", "X4")]
X_validation <- price_reduction[indexes_validation, c("X2", "X3", "X4")]

# Variables respuesta
Y_train <- price_reduction$Y[-indexes_validation]
Y_validation <- price_reduction$Y[indexes_validation]
```

### Funcion para encontrar los K vecinos mas cercanos

```{r}
k_nearest_neighbors_prediction <- function(k, x, dataset_train, response_train) {
  
  # Calculamos la distancia entre x y cada elemento del dataset_train
  mix_x_data_train <- rbind(x, dataset_train)
  
  # Computamos la matriz de distancias, usando distancia euclidiana
  distances <- as.matrix(dist(mix_x_data_train))
  
  # Seleccionamos la distancia a la primera fila
  distance_vector <- distances[1, -1] 
  
  # Ordenamos el vector de manera descendente y conservamos los indices
  ordered_distance_vector <- sort(distance_vector, index.return = TRUE)
  
  # Extraemos los primeros k-vecinos mas cercanos
  k_neighbors_index <- ordered_distance_vector$ix[1:k]
  
  # Promediamos la variable respuesta
  prediction <- mean(response_train[k_neighbors_index])
  
  return(prediction)
}
```

### Predicción de los datos de validación

```{r}
# Recuerde que el argumento 1 significa que se aplica la función a las filas
Y_validation_prediction <- apply(
  X_validation, 1, 
  k_nearest_neighbors_prediction, k = 1, dataset_train = X_train, response_train = Y_train
)
```

Calculamos el MSE

```{r}
MSE <- mean((Y_validation_prediction - Y_validation) ^ 2)
```

### Predicción vs datos reales

```{r}
plot(Y_validation_prediction, Y_validation, xlab = "Costo (predicción) [Pesos]", ylab = "Costo (real) [Pesos]")
abline(a = 0, b = 1)
```

### Normalización de los datos y gráficas del K óptimo

```{r}
# Normalizamos los datos para evitar efectos de escala
price_reduction_scale = as.data.frame(scale(price_reduction, center = TRUE, scale = TRUE))

number_records <- dim(price_reduction_scale)[1]
number_records_validation <- round(number_records * 0.25)

indexes_validation <- sample(1:number_records, number_records_validation, replace = FALSE)

# Variables independientes
X_train_scale <- price_reduction_scale[-indexes_validation, c("X2", "X3", "X4")]
X_validation_scale <- price_reduction_scale[indexes_validation, c("X2", "X3", "X4")]

# Variables respuesta
Y_train_scale <- price_reduction_scale$Y[-indexes_validation]
Y_validation_scale <- price_reduction_scale$Y[indexes_validation]

# All possible k tested
possible_k <- 1:10

# Function that computes mse given k
calculate_mse <- function(k_test, validation_dataset, dataset_train, response_train, response_validation) {
  
  Y_validation_prediction_result <- apply(
    validation_dataset,
    1,
    k_nearest_neighbors_prediction,
    k = k_test,
    dataset_train,
    response_train
  )

  MSE <- mean((Y_validation_prediction_result - response_validation) ^ 2)

  return(MSE)
}

results <- lapply(
  possible_k,
  calculate_mse,
  validation_dataset = X_validation_scale,
  dataset_train = X_train_scale,
  response_train = Y_train_scale,
  response_validation = Y_validation_scale
)

plot(possible_k, results, type = "l", xlab = "K", ylab = "MSE", lwd = 2)
```

## KNN con caret

```{r}
library(caret)

# Determinamos el control de entrenamiento a validación cruzada
train_ctrl = trainControl(method = "cv", number = 10)

# Entrenamos el modelo
  knn_model = train(Y ~ .,
                  data = price_reduction,
                  method = "knn",
                  tuneGrid = expand.grid(k = 1:30),
                  trControl = train_ctrl,
                  preProcess = c("scale", "center"))


plot(knn_model)

#Comparación entre ambas predicciones
prediction_caret <- predict(knn_model, newdata = X_validation[1, ])

prediction_model = k_nearest_neighbors_prediction(
  k = 5,
  x = X_validation[1, ],
  dataset_train = X_train,
  response_train = Y_train
) #Prediccion sin escalamiento

```

## Selección de dos variables

Entre $X_1$, $X_2$ y $X_3$ seleccionaremos las dos variables que reducen el MSE. Para ello, entrenaremos varios modelos para cada pareja de variables $(X_1, X_2)$, $(X_1, X_3)$ y $(X_2, X_3)$ y extraemos la pareja con el mejor MSE.

```{r}
# Determinamos el control de entrenamiento a validación cruzada
train_ctrl = trainControl(method = "cv", number = 10)

# Entrenamos el modelo
knn_model_x2_x3 = train(Y ~ X2 + X3,
                  data = price_reduction,
                  method = "knn",
                  tuneGrid = expand.grid(k = 1:30),
                  trControl = train_ctrl,
                  preProcess = c("scale", "center"))

knn_model_x2_x4 = train(Y ~ X2 + X4,
                  data = price_reduction,
                  method = "knn",
                  tuneGrid = expand.grid(k = 1:30),
                  trControl = train_ctrl,
                  preProcess = c("scale", "center"))

knn_model_x3_x4 = train(Y ~ X3 + X4, 
                  data = price_reduction,
                  method = "knn",
                  tuneGrid = expand.grid(k = 1:30),
                  trControl = train_ctrl,
                  preProcess = c("scale", "center"))

print(paste("MSE (X2, X3): ", knn_model_x2_x3$results$RMSE[knn_model_x2_x3$bestTune$k]))

print(paste("MSE (X2, X4): ", knn_model_x2_x4$results$RMSE[knn_model_x2_x4$bestTune$k]))

print(paste("MSE (X3, X4): ", knn_model_x3_x4$results$RMSE[knn_model_x3_x4$bestTune$k]))

knn_model

results_comparison <-
  rbind(
    data.frame(
      k = 1:10,
      MSE = knn_model_x2_x3$results$RMSE[1:10],
      type = "(X2, X3)"
    ),
    
    data.frame(
      k = 1:10,
      MSE = knn_model_x2_x4$results$RMSE[1:10],
      type = "(X2, X4)"
    ),
        
    data.frame(
      k = 1:10,
      MSE = knn_model_x3_x4$results$RMSE[1:10],
      type = "(X3, X4)"
    )
  )

ggplot(data = results_comparison, aes(x = k, y = MSE)) +
  geom_line(aes(color = type)) 
```
## Seleccion del mejor modelo con una sola variable

```{r}
# Determinamos el control de entrenamiento a validación cruzada
train_ctrl = trainControl(method = "cv", number = 10)

# Entrenamos el modelo
knn_model_x2 = train(Y ~ X2,
                  data = price_reduction,
                  method = "knn",
                  tuneGrid = expand.grid(k = 1:30),
                  trControl = train_ctrl,
                  preProcess = c("scale", "center"))

# Entrenamos el modelo
knn_model_x3 = train(Y ~ X3,
                  data = price_reduction,
                  method = "knn",
                  tuneGrid = expand.grid(k = 1:30),
                  trControl = train_ctrl,
                  preProcess = c("scale", "center"))

# Entrenamos el modelo
knn_model_x4 = train(Y ~ X4,
                  data = price_reduction,
                  method = "knn",
                  tuneGrid = expand.grid(k = 1:30),
                  trControl = train_ctrl,
                  preProcess = c("scale", "center"))

print(paste("MSE (X2): ", knn_model_x2$results$RMSE[knn_model_x2$bestTune$k]))
print(paste("MSE (X3): ", knn_model_x3$results$RMSE[knn_model_x3$bestTune$k]))
print(paste("MSE (X4): ", knn_model_x4$results$RMSE[knn_model_x4$bestTune$k]))
```

## Curvas de los modelos

### Para una variable

```{r}
values_x <- data.frame(X2 = seq(1, 100, by = 0.1))

result <- predict(knn_model_x2, values_x)

plot(values_x$X2, result, type = "l", xlab = "X2", ylab = "Precio [Pesos]")
```

### Para dos variables
```{r}

evaluate <- function(x, y, model) {
  data_evaluate = data.frame(X2 = x, X3 = y)
  prediction <- predict(model, newdata = data_evaluate)
  return(prediction)
}

values_x2 <- seq(1, 100, by = 5)
values_x3 <- seq(1, 100, by = 5)

matrix <- outer(values_x2, values_x3, evaluate, knn_model_x2_x3)

persp(values_x2, values_x3, matrix)
```
```{r}
library(plotly)

values_x3 <- seq(min(price_reduction$X3), max(price_reduction$X3), by = 1)
values_x4 <- 0:10

matrix_expand <- expand.grid(values_x3, values_x4)

names(matrix_expand) <- c("X3", "X4")

price_prediction_matrix <- predict(knn_model_x3_x4, newdata = matrix_expand)

z_matrix <- matrix(data = price_prediction_matrix, nrow = length(values_x3), ncol = length(values_x4))

plot_ly(x = values_x4, y = values_x3, z = z_matrix) %>% add_surface()
```


```{r}
predict(knn_model_x3_x4, newdata = data.frame(
  X3 = 44,
  X4 = 59
))

predict(knn_model_x3_x4, newdata = data.frame(
  X3 = 39,
  X4 = 58
))
```



